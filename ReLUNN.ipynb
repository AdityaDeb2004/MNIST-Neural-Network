{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 42000\n",
      "Number of images: 42000\n",
      "First 5 labels: [1, 0, 1, 4, 0]\n",
      "First 5 images: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 188, 255, 94, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 191, 250, 253, 93, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 123, 248, 253, 167, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 80, 247, 253, 208, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 29, 207, 253, 235, 77, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 54, 209, 253, 253, 88, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 93, 254, 253, 238, 170, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 210, 254, 253, 159, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 209, 253, 254, 240, 81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 27, 253, 253, 254, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 206, 254, 254, 198, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 168, 253, 253, 196, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 203, 253, 248, 76, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 188, 253, 245, 93, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 103, 253, 253, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 89, 240, 253, 195, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 220, 253, 253, 80, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 94, 253, 253, 253, 94, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 89, 251, 253, 250, 131, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 214, 218, 95, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 30, 137, 137, 192, 86, 72, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 86, 250, 254, 254, 254, 254, 217, 246, 151, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 179, 254, 254, 254, 254, 254, 254, 254, 254, 254, 231, 54, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 72, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 104, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 61, 191, 254, 254, 254, 254, 254, 109, 83, 199, 254, 254, 254, 254, 243, 85, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 172, 254, 254, 254, 202, 147, 147, 45, 0, 11, 29, 200, 254, 254, 254, 171, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 174, 254, 254, 89, 67, 0, 0, 0, 0, 0, 0, 128, 252, 254, 254, 212, 76, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 47, 254, 254, 254, 29, 0, 0, 0, 0, 0, 0, 0, 0, 83, 254, 254, 254, 153, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 80, 254, 254, 240, 24, 0, 0, 0, 0, 0, 0, 0, 0, 25, 240, 254, 254, 153, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64, 254, 254, 186, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 166, 254, 254, 224, 12, 0, 0, 0, 0, 0, 0, 0, 0, 14, 232, 254, 254, 254, 29, 0, 0, 0, 0, 0, 0, 0, 0, 0, 75, 254, 254, 254, 17, 0, 0, 0, 0, 0, 0, 0, 0, 18, 254, 254, 254, 254, 29, 0, 0, 0, 0, 0, 0, 0, 0, 0, 48, 254, 254, 254, 17, 0, 0, 0, 0, 0, 0, 0, 0, 2, 163, 254, 254, 254, 29, 0, 0, 0, 0, 0, 0, 0, 0, 0, 48, 254, 254, 254, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 94, 254, 254, 254, 200, 12, 0, 0, 0, 0, 0, 0, 0, 16, 209, 254, 254, 150, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 206, 254, 254, 254, 202, 66, 0, 0, 0, 0, 0, 21, 161, 254, 254, 245, 31, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 60, 212, 254, 254, 254, 194, 48, 48, 34, 41, 48, 209, 254, 254, 254, 171, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 86, 243, 254, 254, 254, 254, 254, 233, 243, 254, 254, 254, 254, 254, 86, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 114, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 239, 86, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 182, 254, 254, 254, 254, 254, 254, 254, 254, 243, 70, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 76, 146, 254, 255, 254, 255, 146, 19, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 141, 139, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 254, 254, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 254, 254, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 254, 254, 106, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 254, 254, 184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 254, 254, 184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 254, 254, 184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 185, 254, 184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 89, 254, 184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 146, 254, 184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 254, 254, 184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 254, 254, 184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 254, 254, 184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 254, 254, 184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 254, 254, 184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 156, 254, 254, 184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 185, 255, 255, 184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 185, 254, 254, 184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 185, 254, 254, 184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 63, 254, 254, 62, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 179, 6, 0, 0, 0, 0, 0, 0, 0, 0, 9, 77, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 247, 17, 0, 0, 0, 0, 0, 0, 0, 0, 27, 202, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 242, 155, 0, 0, 0, 0, 0, 0, 0, 0, 27, 254, 63, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 160, 207, 6, 0, 0, 0, 0, 0, 0, 0, 27, 254, 65, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 127, 254, 21, 0, 0, 0, 0, 0, 0, 0, 20, 239, 65, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 77, 254, 21, 0, 0, 0, 0, 0, 0, 0, 0, 195, 65, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 70, 254, 21, 0, 0, 0, 0, 0, 0, 0, 0, 195, 142, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 56, 251, 21, 0, 0, 0, 0, 0, 0, 0, 0, 195, 227, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 222, 153, 5, 0, 0, 0, 0, 0, 0, 0, 120, 240, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 67, 251, 40, 0, 0, 0, 0, 0, 0, 0, 94, 255, 69, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 234, 184, 0, 0, 0, 0, 0, 0, 0, 19, 245, 69, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 234, 169, 0, 0, 0, 0, 0, 0, 0, 3, 199, 182, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 154, 205, 4, 0, 0, 26, 72, 128, 203, 208, 254, 254, 131, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 61, 254, 129, 113, 186, 245, 251, 189, 75, 56, 136, 254, 73, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 216, 233, 233, 159, 104, 52, 0, 0, 0, 38, 254, 73, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 254, 73, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 254, 73, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 206, 106, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 186, 159, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 209, 101, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 25, 130, 155, 254, 254, 254, 157, 30, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 103, 253, 253, 253, 253, 253, 253, 253, 253, 114, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 208, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 107, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 31, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 215, 101, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 210, 253, 253, 253, 248, 161, 222, 222, 246, 253, 253, 253, 253, 253, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 136, 253, 253, 253, 229, 77, 0, 0, 0, 70, 218, 253, 253, 253, 253, 215, 91, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 214, 253, 253, 253, 195, 0, 0, 0, 0, 0, 104, 224, 253, 253, 253, 253, 215, 29, 0, 0, 0, 0, 0, 0, 0, 0, 0, 116, 253, 253, 253, 247, 75, 0, 0, 0, 0, 0, 0, 26, 200, 253, 253, 253, 253, 216, 4, 0, 0, 0, 0, 0, 0, 0, 0, 254, 253, 253, 253, 195, 0, 0, 0, 0, 0, 0, 0, 0, 26, 200, 253, 253, 253, 253, 5, 0, 0, 0, 0, 0, 0, 0, 0, 254, 253, 253, 253, 99, 0, 0, 0, 0, 0, 0, 0, 0, 0, 25, 231, 253, 253, 253, 36, 0, 0, 0, 0, 0, 0, 0, 0, 254, 253, 253, 253, 99, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 223, 253, 253, 253, 129, 0, 0, 0, 0, 0, 0, 0, 0, 254, 253, 253, 253, 99, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 127, 253, 253, 253, 129, 0, 0, 0, 0, 0, 0, 0, 0, 254, 253, 253, 253, 99, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 139, 253, 253, 253, 90, 0, 0, 0, 0, 0, 0, 0, 0, 254, 253, 253, 253, 99, 0, 0, 0, 0, 0, 0, 0, 0, 0, 78, 248, 253, 253, 253, 5, 0, 0, 0, 0, 0, 0, 0, 0, 254, 253, 253, 253, 216, 34, 0, 0, 0, 0, 0, 0, 0, 33, 152, 253, 253, 253, 107, 1, 0, 0, 0, 0, 0, 0, 0, 0, 206, 253, 253, 253, 253, 140, 0, 0, 0, 0, 0, 30, 139, 234, 253, 253, 253, 154, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 205, 253, 253, 253, 250, 208, 106, 106, 106, 200, 237, 253, 253, 253, 253, 209, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 82, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 209, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 91, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 213, 90, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 18, 129, 208, 253, 253, 253, 253, 159, 129, 90, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to hold the labels and pixel data\n",
    "labels = []\n",
    "images = []\n",
    "\n",
    "# Open and read the CSV file\n",
    "with open('train.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    # Skip the header row\n",
    "    next(reader)\n",
    "    \n",
    "    # Iterate through each row in the CSV file\n",
    "    for row in reader:\n",
    "        # The first column is the label\n",
    "        labels.append(int(row[0]))\n",
    "        # The rest of the columns are the pixels\n",
    "        images.append([int(pixel) for pixel in row[1:]])\n",
    "\n",
    "# Output the lengths of the lists to confirm they have been populated correctly\n",
    "print(f'Number of labels: {len(labels)}')\n",
    "print(f'Number of images: {len(images)}')\n",
    "\n",
    "# Optionally print the first few elements to check\n",
    "print('First 5 labels:', labels[:5])\n",
    "print('First 5 images:', images[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7372549019607844, 1.0, 0.3686274509803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7490196078431373, 0.9803921568627451, 0.9921568627450981, 0.36470588235294116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4823529411764706, 0.9725490196078431, 0.9921568627450981, 0.6549019607843137, 0.0392156862745098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3137254901960784, 0.9686274509803922, 0.9921568627450981, 0.8156862745098039, 0.050980392156862744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11372549019607843, 0.8117647058823529, 0.9921568627450981, 0.9215686274509803, 0.30196078431372547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21176470588235294, 0.8196078431372549, 0.9921568627450981, 0.9921568627450981, 0.34509803921568627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.36470588235294116, 0.996078431372549, 0.9921568627450981, 0.9333333333333333, 0.6666666666666666, 0.06666666666666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09019607843137255, 0.8235294117647058, 0.996078431372549, 0.9921568627450981, 0.6235294117647059, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06274509803921569, 0.8196078431372549, 0.9921568627450981, 0.996078431372549, 0.9411764705882353, 0.3176470588235294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10588235294117647, 0.9921568627450981, 0.9921568627450981, 0.996078431372549, 0.050980392156862744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0784313725490196, 0.807843137254902, 0.996078431372549, 0.996078431372549, 0.7764705882352941, 0.027450980392156862, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6588235294117647, 0.9921568627450981, 0.9921568627450981, 0.7686274509803922, 0.027450980392156862, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0784313725490196, 0.796078431372549, 0.9921568627450981, 0.9725490196078431, 0.2980392156862745, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08627450980392157, 0.7372549019607844, 0.9921568627450981, 0.9607843137254902, 0.36470588235294116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.403921568627451, 0.9921568627450981, 0.9921568627450981, 0.7490196078431373, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.34901960784313724, 0.9411764705882353, 0.9921568627450981, 0.7647058823529411, 0.09803921568627451, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.058823529411764705, 0.8627450980392157, 0.9921568627450981, 0.9921568627450981, 0.3137254901960784, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3686274509803922, 0.9921568627450981, 0.9921568627450981, 0.9921568627450981, 0.3686274509803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.34901960784313724, 0.984313725490196, 0.9921568627450981, 0.9803921568627451, 0.5137254901960784, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8392156862745098, 0.8549019607843137, 0.37254901960784315, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n"
     ]
    }
   ],
   "source": [
    "# Make a list of tuples of dataset: ([list of pixels in image], actual digit value)\n",
    "combined_data = []\n",
    "for label, image in zip(labels, images):\n",
    "    pixels = [(pixel / 255.0) for pixel in image]\n",
    "    combined_data.append((label, pixels))\n",
    "print(combined_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(combined_data)\n",
    "train_data = combined_data[:32000]\n",
    "test_data = combined_data[32000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNetwork(object):\n",
    "    def __init__(self, layers):\n",
    "        # Number of layers in size of the input list\n",
    "        self.num_layers = len(layers)\n",
    "\n",
    "        # Each element in the list corresponds to number of neurons in that layer\n",
    "        self.layers = layers\n",
    "\n",
    "        self.biases = []\n",
    "\n",
    "        # Randomly generate the weights and biases\n",
    "        # For biases, just make it all start out at 0.1\n",
    "        for i in range(1, len(layers)):\n",
    "            bias = []\n",
    "            for j in range(layers[i]):\n",
    "                bias.append(0.0)\n",
    "            self.biases.append(bias)\n",
    "        \n",
    "\n",
    "        # Randomly generate a (size(layer + 1) x size(layer) matrix of weights using Xavier Initialization method\n",
    "        # Note, self.weights is not a numpy array. Later on we will treat each individual array within it as a numpy array, but not actually\n",
    "        # convert them into a numpy array\n",
    "        self.weights = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            weight = [[random.gauss(0.0, math.sqrt(2 / (layers[i] + layers[i+1]))) for _ in range(layers[i])] for _ in range(layers[i + 1])]\n",
    "            self.weights.append(weight)\n",
    "\n",
    "\n",
    "        self.activations = []\n",
    "        self.sig_activations = []\n",
    "\n",
    "\n",
    "    # Method to calculate the output layer given the input layer\n",
    "    #\n",
    "    # inputLayer: a list of values indicating the initial values of input layer\n",
    "    def calculateOutputLayer(self, inputLayer):\n",
    "        # Have variable currLayer keeping track of values of whatever layer we are on in loop\n",
    "        currLayer = inputLayer\n",
    "        activations = []\n",
    "        sig_activations = []\n",
    "\n",
    "        sig_activations.append(inputLayer)\n",
    "        # Loop over the length of bias list\n",
    "        for b in range(len(self.biases)):\n",
    "            # Create a temporary nextLayer list to store the calculated values of weighted sum and the sigmoid of weighted sum\n",
    "            nextLayer = np.dot(self.weights[b], np.array(currLayer).reshape(len(currLayer), 1))\n",
    "            \n",
    "            sigNextLayer = relu(nextLayer)\n",
    "            currLayer = sigNextLayer\n",
    "            \n",
    "            # Append sigmoid layer and normal layer to respective lists\n",
    "            sig_activations.append(list(sigNextLayer.reshape(len(sigNextLayer))))\n",
    "            activations.append(list(nextLayer.reshape(len(nextLayer))))\n",
    "\n",
    "        self.activations = activations\n",
    "        self.sig_activations = sig_activations\n",
    "\n",
    "        self.sig_activations[-1] = list(np.array(softmax(activations[-1])))\n",
    "\n",
    "        return currLayer\n",
    "                \n",
    "    # Method to calculate the weighted sum\n",
    "    #\n",
    "    # weights: A list of weights for a specific neuron\n",
    "    # currLayer: The list of neuron activation values of current layer\n",
    "    def weightedSum(self, weights, currLayer):\n",
    "        sum = 0\n",
    "        # Loop over the length of weights\n",
    "        for i in range(len(weights)):\n",
    "            # Weighted sum calculation\n",
    "            sum += weights[i] * currLayer[i]\n",
    "        return sum\n",
    "    \n",
    "    # Method to start the training of the network\n",
    "    #\n",
    "    # train_data: The input data to train on\n",
    "    # batch_size: The size of each batch within the overall input data\n",
    "    def startTrain(self, train_data, batch_size, iterations):\n",
    "\n",
    "        print(\"Starting Training\", end='\\n\\n')\n",
    "        for i in range(iterations):\n",
    "            print(f\"\\n\\n\\nStarting Iteration {i}\\n\\n\\n\")\n",
    "            random.shuffle(train_data)\n",
    "\n",
    "            # Making a list to store all the batches\n",
    "            batches = []\n",
    "            # Loop over training data and create batches of (batch_size)\n",
    "            for j in range(0, len(train_data), batch_size):\n",
    "                batch = train_data[j:j + batch_size]\n",
    "                batches.append(batch)\n",
    "\n",
    "            # For each batch created, do backprop and update all parameters and calculate the average cost function of each batch\n",
    "            counter = 1\n",
    "            for mini_batch in batches:\n",
    "                print(f\"Batch Number {counter}\")\n",
    "                avgCost = self.update_parameters(mini_batch)\n",
    "                print(f\"Cost: {avgCost}\")\n",
    "                counter += 1\n",
    "        \n",
    "\n",
    "    # Method to update the weights and biases within a network\n",
    "    #\n",
    "    # batch: the mini batch of the training data to do backprop on. Each batch is a list of tuples, with each tuple containing\n",
    "    #        the actual value of the image, and a list of pixels representing the image\n",
    "    def update_parameters(self, batch):\n",
    "        avgCost = 0\n",
    "\n",
    "        # The total lists contains the average way to shift weights and biases\n",
    "        total_delta_bias = []\n",
    "        # Create a bias list (similar to self.biases) with all 0's\n",
    "        for i in range(len(self.biases)):\n",
    "            total_delta_bias.append([0 for j in range(len(self.biases[i]))])\n",
    "\n",
    "        total_delta_weight = []\n",
    "        # Create a weight list (similar to self.weights) with all 0's\n",
    "        for i in range(len(self.weights)):\n",
    "            weight = []\n",
    "            for j in range(len(self.weights[i])):\n",
    "                weight.append([0 for k in range(len(self.weights[i][j]))])\n",
    "            total_delta_weight.append(weight)\n",
    "\n",
    "        # For each tuple in the list do backprop (remember each tuple: (number, [list of pixels]) )\n",
    "        for actualValue, pixels in batch:\n",
    "            delta_bias, delta_weight = self.backprop(actualValue, pixels)\n",
    "\n",
    "            # Keep track of average cost in batch\n",
    "            avgCost += costFunction(self.sig_activations[-1], actualValue)\n",
    "\n",
    "            # Add up the individual bias and weight gradients of each tuple in batch\n",
    "            total_delta_bias = [(np.array(totBias) + np.array(bias)).tolist() for totBias, bias in zip(total_delta_bias, delta_bias)]\n",
    "            total_delta_weight = [(np.array(totWeight) + np.array(weight)).tolist() for totWeight, weight in zip(total_delta_weight, delta_weight)]\n",
    "\n",
    "\n",
    "        # Update the weights and biases with the average gradient change to each\n",
    "        self.weights = [(np.array(currWeight) + (np.array(weight) / (100 * len(weight)))).tolist() for currWeight, weight in zip(self.weights, total_delta_weight)]\n",
    "        self.biases = [(np.array(currBias) + (np.array(bias) / (100 * len(batch)))).tolist() for currBias, bias in zip(self.biases, total_delta_bias)]\n",
    "\n",
    "        # Return average cost for statistical purposes\n",
    "        return (avgCost / len(batch))\n",
    "\n",
    "\n",
    "    # Method to do backpropagation when training\n",
    "    #\n",
    "    # label: The actual value represented in the image\n",
    "    # pixels: The list of pixels representing the image\n",
    "    def backprop(self, label, pixels):\n",
    "        # Calculate the values of each layer for image\n",
    "        self.calculateOutputLayer(pixels)\n",
    "\n",
    "        # The delta lists contains the way to shift weights and biases to recognize that image\n",
    "        delta_bias = []\n",
    "        # Create a bias list (similar to self.biases) with all 0's\n",
    "        for i in range(len(self.biases)):\n",
    "            delta_bias.append([0 for j in range(len(self.biases[i]))])\n",
    "\n",
    "        delta_weight = []\n",
    "        # Create a weight list (similar to self.weights) with all 0's\n",
    "        for i in range(len(self.weights)):\n",
    "            weight = []\n",
    "            for j in range(len(self.weights[i])):\n",
    "                weight.append([0 for k in range(len(self.weights[i][j]))])\n",
    "            delta_weight.append(weight)\n",
    "\n",
    "        # Calculate the derivative of cost function and activation function (sigmoid function) when back propagating\n",
    "        costPrime = costFunctionPrime(self.sig_activations[-1], label)\n",
    "        sigPrime = reluPrime(self.activations[-1])\n",
    "\n",
    "        # Delta represents the multiplied costPrime and sigPrime lists. This is the constant term needed for all of the backprop\n",
    "        # calculations \n",
    "        delta = (np.array(costPrime) * np.array(sigPrime)).reshape(len(costPrime), 1)\n",
    "        delta_bias[-1] = delta.reshape(len(delta)).tolist()\n",
    "        delta_weight[-1] = np.dot(delta, np.array(self.sig_activations[-2]).reshape(1, len(self.sig_activations[-2]))).tolist()\n",
    "\n",
    "        # Loop over each layer starting from output layer, and calculate the backprop needed for the weights and biases of each layer\n",
    "        for layer in range(2, self.num_layers):\n",
    "            delta = np.dot(np.array(self.weights[-layer + 1]).reshape(len(self.weights[-layer + 1][0]), len(self.weights[-layer + 1])), delta)\n",
    "            delta *= np.array(reluPrime(self.activations[-layer])).reshape(len(self.activations[-layer]), 1)\n",
    "\n",
    "            # Calculate the change needed in the weights and biases of each layer\n",
    "            delta_bias[-layer] = delta.reshape(len(delta)).tolist()\n",
    "            delta_weight[-layer] = np.dot(delta, np.array(self.sig_activations[-layer - 1]).reshape(1, len(self.sig_activations[-layer - 1]))).tolist()\n",
    "            \n",
    "        return (delta_bias, delta_weight)\n",
    "    \n",
    "\n",
    "    def accuracy(self, inputs):\n",
    "        totalInputs = len(inputs)\n",
    "        totalRight = 0\n",
    "\n",
    "        for value, pixels in inputs:\n",
    "            totalRight += self.isCorrect(value, pixels)\n",
    "\n",
    "        percentAcc = float('{:.2f}'.format(totalRight / totalInputs))\n",
    "\n",
    "        print(f\"Total Accuracy: {percentAcc}\")\n",
    "        return percentAcc\n",
    "\n",
    "    def isCorrect(self, value, pixels):\n",
    "        self.calculateOutputLayer(pixels)\n",
    "\n",
    "        max = np.argmax(self.sig_activations[-1])\n",
    "\n",
    "        if max == value:\n",
    "            return 1\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "# Method to display the cost function output\n",
    "def costFunction(outputLayer, desiredOutput):\n",
    "    sum = 0\n",
    "    for i in range(len(outputLayer)):\n",
    "        if (i == desiredOutput):\n",
    "            sum += (1 - outputLayer[i])**2\n",
    "        else:\n",
    "            sum += (0 - outputLayer[i])**2\n",
    "    return sum\n",
    "\n",
    "def costFunctionPrime(outputLayer, desiredOutput):\n",
    "    #result = [activation - desiredOutput for activation in outputLayer]\n",
    "    result = []\n",
    "    for i in range(len(outputLayer)):\n",
    "        if (i == desiredOutput):\n",
    "            result.append(1 - outputLayer[i])\n",
    "        else:\n",
    "            result.append(0 - outputLayer[i])\n",
    "    return result\n",
    "\n",
    "\n",
    "# The sigmoid function\n",
    "def sigmoid(activation):\n",
    "    return 1 / (1 + np.exp(-activation))\n",
    "\n",
    "# The derivative of the sigmoid function\n",
    "def sigmoidPrime(activation):\n",
    "    return (sigmoid(activation) * (1 - sigmoid(activation)))\n",
    "\n",
    "def sigmoidPrimeList(activation_list):\n",
    "    return [sigmoidPrime(activation) for activation in activation_list]\n",
    "\n",
    "def relu(activation_list):\n",
    "    return np.maximum(0, activation_list)\n",
    "\n",
    "def reluPrime(activation_list):\n",
    "    return [np.where(activation > 0, 1, 0) for activation in activation_list]\n",
    "\n",
    "def softmax(activation_list):\n",
    "    return (np.exp(activation_list - np.max(activation_list))) / ((np.exp(activation_list - np.max(activation_list))).sum(axis=0, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Starting Iteration 0\n",
      "\n",
      "\n",
      "\n",
      "Batch Number 1\n",
      "Cost: 0.9050419227921448\n",
      "Batch Number 2\n",
      "Cost: 0.8953885748659691\n",
      "Batch Number 3\n",
      "Cost: 0.9078747928748178\n",
      "Batch Number 4\n",
      "Cost: 0.9019361453877396\n",
      "Batch Number 5\n",
      "Cost: 0.913247635901951\n",
      "Batch Number 6\n",
      "Cost: 0.9036110465521994\n",
      "Batch Number 7\n",
      "Cost: 0.9086692848505072\n",
      "Batch Number 8\n",
      "Cost: 0.9013557096694557\n",
      "Batch Number 9\n",
      "Cost: 0.9008967314258334\n",
      "Batch Number 10\n",
      "Cost: 0.8919172455906782\n",
      "Batch Number 11\n",
      "Cost: 0.9073371575625037\n",
      "Batch Number 12\n",
      "Cost: 0.9100749636998813\n",
      "Batch Number 13\n",
      "Cost: 0.9061021481819178\n",
      "Batch Number 14\n",
      "Cost: 0.9085677326482164\n",
      "Batch Number 15\n",
      "Cost: 0.9056796709133923\n",
      "Batch Number 16\n",
      "Cost: 0.904189109064227\n",
      "Batch Number 17\n",
      "Cost: 0.9122588584805007\n",
      "Batch Number 18\n",
      "Cost: 0.9066209721180039\n",
      "Batch Number 19\n",
      "Cost: 0.9124993732616348\n",
      "Batch Number 20\n",
      "Cost: 0.9154271956787947\n",
      "Batch Number 21\n",
      "Cost: 0.9224131805044664\n",
      "Batch Number 22\n",
      "Cost: 0.9174194844896152\n",
      "Batch Number 23\n",
      "Cost: 0.9223942868971503\n",
      "Batch Number 24\n",
      "Cost: 0.9181528257883594\n",
      "Batch Number 25\n",
      "Cost: 0.9280293249025907\n",
      "Batch Number 26\n",
      "Cost: 0.9222585583228704\n",
      "Batch Number 27\n",
      "Cost: 0.927651521417451\n",
      "Batch Number 28\n",
      "Cost: 0.906771918161065\n",
      "Batch Number 29\n",
      "Cost: 0.9223655783240712\n",
      "Batch Number 30\n",
      "Cost: 0.9219888960192487\n",
      "Batch Number 31\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m networkTest \u001b[38;5;241m=\u001b[39m BasicNetwork([\u001b[38;5;241m784\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m \u001b[43mnetworkTest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m networkTest\u001b[38;5;241m.\u001b[39maccuracy(test_data)\n",
      "Cell \u001b[1;32mIn[6], line 96\u001b[0m, in \u001b[0;36mBasicNetwork.startTrain\u001b[1;34m(self, train_data, batch_size, iterations)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mini_batch \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch Number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcounter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 96\u001b[0m     avgCost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmini_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCost: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavgCost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     98\u001b[0m     counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[6], line 131\u001b[0m, in \u001b[0;36mBasicNetwork.update_parameters\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;66;03m# Add up the individual bias and weight gradients of each tuple in batch\u001b[39;00m\n\u001b[0;32m    130\u001b[0m     total_delta_bias \u001b[38;5;241m=\u001b[39m [(np\u001b[38;5;241m.\u001b[39marray(totBias) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39marray(bias))\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mfor\u001b[39;00m totBias, bias \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(total_delta_bias, delta_bias)]\n\u001b[1;32m--> 131\u001b[0m     total_delta_weight \u001b[38;5;241m=\u001b[39m [\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotWeight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m totWeight, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(total_delta_weight, delta_weight)]\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Update the weights and biases with the average gradient change to each\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m [(np\u001b[38;5;241m.\u001b[39marray(currWeight) \u001b[38;5;241m+\u001b[39m (np\u001b[38;5;241m.\u001b[39marray(weight) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(weight))))\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mfor\u001b[39;00m currWeight, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, total_delta_weight)]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "networkTest = BasicNetwork([784, 16, 16, 10])\n",
    "\n",
    "networkTest.startTrain(train_data, 100, 1)\n",
    "\n",
    "networkTest.accuracy(test_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
