{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to hold the labels and pixel data\n",
    "labels = []\n",
    "images = []\n",
    "\n",
    "# Open and read the CSV file\n",
    "with open('train.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    # Skip the header row\n",
    "    next(reader)\n",
    "    \n",
    "    # Iterate through each row in the CSV file\n",
    "    for row in reader:\n",
    "        # The first column is the label\n",
    "        labels.append(int(row[0]))\n",
    "        # The rest of the columns are the pixels\n",
    "        images.append([int(pixel) for pixel in row[1:]])\n",
    "\n",
    "# Output the lengths of the lists to confirm they have been populated correctly\n",
    "print(f'Number of labels: {len(labels)}')\n",
    "print(f'Number of images: {len(images)}')\n",
    "\n",
    "# Optionally print the first few elements to check\n",
    "print('First 5 labels:', labels[:5])\n",
    "print('First 5 images:', images[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of tuples of dataset: ([list of pixels in image], actual digit value)\n",
    "combined_data = []\n",
    "for label, image in zip(labels, images):\n",
    "    pixels = [(pixel / 255.0) for pixel in image]\n",
    "    combined_data.append((label, pixels))\n",
    "print(combined_data[0])\n",
    "\n",
    "#combined_data = [(label, image) for image, label in zip(images, labels)]\n",
    "#print(combined_data[0])\n",
    "#print(len(combined_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(combined_data)\n",
    "train_data = combined_data[:32000]\n",
    "test_data = combined_data[32000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNetwork(object):\n",
    "    def __init__(self, layers):\n",
    "        # Number of layers in size of the input list\n",
    "        self.num_layers = len(layers)\n",
    "\n",
    "        # Each element in the list corresponds to number of neurons in that layer\n",
    "        self.layers = layers\n",
    "\n",
    "        self.biases = []\n",
    "\n",
    "        # Randomly generate the weights and biases\n",
    "        # For biases, just make it all start out at 0.1\n",
    "        for i in range(1, len(layers)):\n",
    "            bias = []\n",
    "            for j in range(layers[i]):\n",
    "                bias.append(0.0)\n",
    "            self.biases.append(np.array(bias).reshape(len(bias), 1))\n",
    "        \n",
    "\n",
    "        # Randomly generate a (size(layer + 1) x size(layer) matrix of weights using Xavier Initialization method\n",
    "        # Note, self.weights is not a numpy array. Later on we will treat each individual array within it as a numpy array, but not actually\n",
    "        # convert them into a numpy array\n",
    "        self.weights = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            weight = [[random.gauss(0.0, math.sqrt(2 / (layers[i] + layers[i+1]))) for _ in range(layers[i])] for _ in range(layers[i + 1])]\n",
    "            self.weights.append(np.array(weight))\n",
    "\n",
    "\n",
    "        self.activations = []\n",
    "        self.sig_activations = []\n",
    "\n",
    "\n",
    "        # Method to calculate the output layer given the input layer\n",
    "    #\n",
    "    # inputLayer: a list of values indicating the initial values of input layer\n",
    "    def calculateOutputLayer(self, inputLayer):\n",
    "        # Have variable currLayer keeping track of values of whatever layer we are on in loop\n",
    "        currLayer = np.array(inputLayer).reshape(len(inputLayer), 1)\n",
    "        activations = []\n",
    "        sig_activations = []\n",
    "\n",
    "        sig_activations.append(currLayer)\n",
    "        # Loop over the length of bias list\n",
    "        for b in range(len(self.biases)):\n",
    "            # Create a temporary nextLayer list to store the calculated values of weighted sum and the sigmoid of weighted sum\n",
    "            nextLayer = np.dot(self.weights[b], currLayer) + self.biases[b]\n",
    "\n",
    "            sigNextLayer = sigmoid(nextLayer)\n",
    "\n",
    "            currLayer = sigNextLayer\n",
    "            \n",
    "            # Append sigmoid layer and normal layer to respective lists\n",
    "            sig_activations.append(sigNextLayer)\n",
    "            activations.append(nextLayer)\n",
    "\n",
    "        self.activations = activations\n",
    "        self.sig_activations = sig_activations\n",
    "        return currLayer\n",
    "    \n",
    "    # Method to start the training of the network\n",
    "    #\n",
    "    # train_data: The input data to train on\n",
    "    # batch_size: The size of each batch within the overall input data\n",
    "    def startTrain(self, train_data, batch_size, iterations):\n",
    "\n",
    "        print(\"Starting Training\", end='\\n\\n')\n",
    "        for i in range(iterations):\n",
    "            print(f\"\\n\\n\\nStarting Iteration {i}\\n\\n\\n\")\n",
    "            random.shuffle(train_data)\n",
    "\n",
    "            # Making a list to store all the batches\n",
    "            batches = []\n",
    "            # Loop over training data and create batches of (batch_size)\n",
    "            for j in range(0, len(train_data), batch_size):\n",
    "                batch = train_data[j:j + batch_size]\n",
    "                batches.append(batch)\n",
    "\n",
    "            # For each batch created, do backprop and update all parameters and calculate the average cost function of each batch\n",
    "            counter = 1\n",
    "            for mini_batch in batches:\n",
    "                print(f\"Batch Number {counter}\")\n",
    "                avgCost = self.update_parameters(mini_batch)\n",
    "                print(f\"Cost: {avgCost}\")\n",
    "                counter += 1\n",
    "        \n",
    "    # Method to update the weights and biases within a network\n",
    "    #\n",
    "    # batch: the mini batch of the training data to do backprop on. Each batch is a list of tuples, with each tuple containing\n",
    "    #        the actual value of the image, and a list of pixels representing the image\n",
    "    def update_parameters(self, batch):\n",
    "        avgCost = 0\n",
    "\n",
    "        # The total lists contains the average way to shift weights and biases\n",
    "        total_delta_bias = []\n",
    "        # Create a bias list (similar to self.biases) with all 0's\n",
    "        for i in range(len(self.biases)):\n",
    "            total_delta_bias.append(np.array([0 for j in range(len(self.biases[i]))]).reshape(len(self.biases[i]), 1))\n",
    "\n",
    "        total_delta_weight = []\n",
    "        # Create a weight list (similar to self.weights) with all 0's\n",
    "        for i in range(len(self.weights)):\n",
    "            weight = []\n",
    "            for j in range(len(self.weights[i])):\n",
    "                weight.append([0 for k in range(len(self.weights[i][j]))])\n",
    "            total_delta_weight.append(np.array(weight))\n",
    "\n",
    "        # For each tuple in the list do backprop (remember each tuple: (number, [list of pixels]) )\n",
    "        for actualValue, pixels in batch:\n",
    "            delta_bias, delta_weight = self.backprop(actualValue, pixels)\n",
    "\n",
    "            # Keep track of average cost in batch\n",
    "            avgCost += costFunction(self.sig_activations[-1], actualValue)\n",
    "\n",
    "            # Add up the individual bias and weight gradients of each tuple in batch\n",
    "            total_delta_bias = [(totBias + bias) for totBias, bias in zip(total_delta_bias, delta_bias)]\n",
    "            total_delta_weight = [(totWeight + weight) for totWeight, weight in zip(total_delta_weight, delta_weight)]\n",
    "\n",
    "\n",
    "        # Update the weights and biases with the average gradient change to each\n",
    "        self.weights = [(currWeight + (weight / (10 * len(weight)))) for currWeight, weight in zip(self.weights, total_delta_weight)]\n",
    "        self.biases = [(currBias + (bias / (10 * len(batch)))) for currBias, bias in zip(self.biases, total_delta_bias)]\n",
    "\n",
    "        # Return average cost for statistical purposes\n",
    "        return (avgCost / len(batch))\n",
    "    \n",
    "    # Method to do backpropagation when training\n",
    "    #\n",
    "    # label: The actual value represented in the image\n",
    "    # pixels: The list of pixels representing the image\n",
    "    def backprop(self, label, pixels):\n",
    "        # Calculate the values of each layer for image\n",
    "        self.calculateOutputLayer(pixels)\n",
    "\n",
    "        # The delta lists contains the way to shift weights and biases to recognize that image\n",
    "        delta_bias = []\n",
    "        # Create a bias list (similar to self.biases) with all 0's\n",
    "        for i in range(len(self.biases)):\n",
    "            delta_bias.append(np.array([0 for j in range(len(self.biases[i]))]).reshape(len(self.biases[i]), 1))\n",
    "\n",
    "        delta_weight = []\n",
    "        # Create a weight list (similar to self.weights) with all 0's\n",
    "        for i in range(len(self.weights)):\n",
    "            weight = []\n",
    "            for j in range(len(self.weights[i])):\n",
    "                weight.append([0 for k in range(len(self.weights[i][j]))])\n",
    "            delta_weight.append(np.array(weight))\n",
    "\n",
    "        # Calculate the derivative of cost function and activation function (sigmoid function) when back propagating\n",
    "        costPrime = costFunctionPrime(self.sig_activations[-1], label)\n",
    "        sigPrime = sigmoidPrime(self.activations[-1])\n",
    "\n",
    "        # Delta represents the multiplied costPrime and sigPrime lists. This is the constant term needed for all of the backprop\n",
    "        # calculations \n",
    "        delta = (costPrime * sigPrime)\n",
    "        delta_bias[-1] = delta\n",
    "        delta_weight[-1] = np.dot(delta, (self.sig_activations[-2]).reshape(1, len(self.sig_activations[-2])))\n",
    "\n",
    "        # Loop over each layer starting from output layer, and calculate the backprop needed for the weights and biases of each layer\n",
    "        for layer in range(2, self.num_layers):\n",
    "            delta = np.dot((self.weights[-layer + 1]).reshape(len(self.weights[-layer + 1][0]), len(self.weights[-layer + 1])), delta)\n",
    "            delta *= sigmoidPrime(self.activations[-layer]).reshape(len(self.activations[-layer]), 1)\n",
    "\n",
    "            # Calculate the change needed in the weights and biases of each layer\n",
    "            delta_bias[-layer] = delta\n",
    "            delta_weight[-layer] = np.dot(delta, (self.sig_activations[-layer - 1]).reshape(1, len(self.sig_activations[-layer - 1])))\n",
    "            \n",
    "        return (delta_bias, delta_weight)\n",
    "    \n",
    "    def accuracy(self, inputs):\n",
    "        totalInputs = len(inputs)\n",
    "        totalRight = 0\n",
    "\n",
    "        for value, pixels in inputs:\n",
    "            totalRight += self.isCorrect(value, pixels)\n",
    "\n",
    "        percentAcc = float('{:.2f}'.format(totalRight / totalInputs))\n",
    "\n",
    "        print(f\"Total Accuracy: {percentAcc}\")\n",
    "        return percentAcc\n",
    "\n",
    "    def isCorrect(self, value, pixels):\n",
    "        self.calculateOutputLayer(pixels)\n",
    "\n",
    "        max = np.argmax(self.sig_activations[-1])\n",
    "\n",
    "        if max == value:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "\n",
    "    # Method to display the cost function output\n",
    "def costFunction(outputLayer, desiredOutput):\n",
    "    sum = 0\n",
    "    for i in range(len(outputLayer)):\n",
    "        if (i == desiredOutput):\n",
    "            sum += (1 - outputLayer[i])**2\n",
    "        else:\n",
    "            sum += (0 - outputLayer[i])**2\n",
    "    return sum\n",
    "\n",
    "def costFunctionPrime(outputLayer, desiredOutput):\n",
    "    #result = [activation - desiredOutput for activation in outputLayer]\n",
    "    result = []\n",
    "    for i in range(len(outputLayer)):\n",
    "        if (i == desiredOutput):\n",
    "            result.append(1 - outputLayer[i])\n",
    "        else:\n",
    "            result.append(0 - outputLayer[i])\n",
    "    return np.array(result)\n",
    "\n",
    "# The sigmoid function\n",
    "def sigmoid(activation):\n",
    "    return 1 / (1 + np.exp(-activation))\n",
    "\n",
    "# The derivative of the sigmoid function\n",
    "def sigmoidPrime(activation):\n",
    "    return (sigmoid(activation) * (1 - sigmoid(activation)))\n",
    "\n",
    "def sigmoidPrimeList(activation_list):\n",
    "    return [sigmoidPrime(activation) for activation in activation_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkTest = BasicNetwork([784, 128, 10])\n",
    "\n",
    "networkTest.startTrain(train_data, 100, 10)\n",
    "\n",
    "networkTest.accuracy(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
